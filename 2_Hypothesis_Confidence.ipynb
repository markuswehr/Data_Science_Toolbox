{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hypothesis Testing and Confidence Intervals\n",
    "### 2.1 Hypothesis Testing\n",
    "\n",
    "**Overview**\n",
    "\n",
    "Hypothesis tests are used to confirm or reject assumptions (hypothesis) based on statistical measures. In applied machine learning, one application of hypothesis tests is to test whether two sets of data are from the same distribution or not (e.g., from the same population) or to test whether a set is normally distributed.<br>\n",
    "\n",
    "There are two hypothesis:<br>\n",
    "- **H0:**Test holds and is failed to be rejected at some level of significance\n",
    "- **H1**: Test does not hold and is rejected at some level of significance\n",
    "\n",
    "To accept or reject H0, we compare the **p-value** to a significance level $\\alpha$ (0.1,0.05,0.01).<br>\n",
    "- If p-value > $\\alpha$: Fail to reject H0\n",
    "- if p-value <= $\\alpha$: Reject H0<br>\n",
    "\n",
    "The p-value is a measure of how likely the data sample would be observed if H0 were true.<br>\n",
    "\n",
    "Though, not all tests return a p-value. Some tests return \"critical values\" and their associated significance levels instead. These are usually non-parametric or distribution-free hypothesis tests.<br>\n",
    "- If test statistic < critical value: Fail to rejetc H0\n",
    "- If test statistic >= critical value: Reject H0<br>\n",
    "\n",
    "The mathematical steps (generalized) are as follows:<br>\n",
    "1. Formt he hypothesis $H_{0}:\\mu = \\mu_{0}$ and the alternative $H_{a}:\\mu > \\mu_{0}$ or $H_{a}:\\mu < \\mu_{0}$ or $H_{a}:\\mu \\neq \\mu_{0}$.<br>\n",
    "2. Calculate the test statistic $z$:<br>\n",
    "$z=\\frac{\\bar{x}-\\mu_{0}}{\\frac{\\sigma}{\\sqrt{n}}}$<br>\n",
    "3. Determine the p-value. Since due to the Central Limit Theorem under $H_{0}$ $Z=\\frac{\\bar{X}-\\mu_{0}}{\\frac{\\sigma}{\\sqrt{n}}}\\sim Norm(0,1)$ the p-value is calculated using normal distribution function. Under $H_{0}$ we should see an event of high probability.\n",
    "\n",
    "\n",
    "**Errors in Statistical Tests**\n",
    "\n",
    "Due to the probabilistic nature of hypothesis tests, they may suggest an outcome and be mistaken. For example, if $\\alpha=0.05$, it suggests that one time in 20 the null hypothesis would be mistakenly rejected or failed to be reject because of the statistical noise in the data sample.<br>\n",
    "\n",
    "Given a small p-value, we either reject H0 correctly or, with a small probability, we incorrectly rejected H0 and it was actually true. The latter is an example of a **false positive**.<br>\n",
    "\n",
    "Alternatively, given a large p-value we fail to reject H0, even though H0 should actually be rejected. This would be called a **false negative**.<br>\n",
    "\n",
    "- **Type I Error:** Incorrect rejection of a true H0 (false positive)\n",
    "- **Type II Error:** Incorrect failure of rejection of a false H0 (false negative)<br>\n",
    "\n",
    "Obviously, you can somewhat control the likelihood of such errors by lowering your significance level (e.g., 5-sigma threshold, often used in physics, which is $3*10^{-7}$).<br>\n",
    "\n",
    "### 2.2 Examples of Hypothesis Tests\n",
    "#### 2.2.1 Variable Distribution Type Tests (Gaussian)\n",
    "\n",
    "This chapter describes three tests and gives basic code in Python to test whether your data is gaussian distributed.<br>\n",
    "\n",
    "**Shapiro-Wilk Test**\n",
    "\n",
    "Assumptions:<br>\n",
    "- Obervations are i.i.d. (independent and identically distributed)<br>\n",
    "\n",
    "Interpretation:<br>\n",
    "- H0: Sample has gaussian distribution\n",
    "- H1: Sample does not have gaussian distribution\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19340917468070984\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "## Normality test\n",
    "data = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = shapiro(data)\n",
    "\n",
    "print(p) # Fail to reject H0 --> Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D'Agostino's $K^{2}$ Test**\n",
    "\n",
    "Assumptions:<br>\n",
    "- Obervastions are i.i.d.\n",
    "\n",
    "Interpretation:<br>\n",
    "- H0: Sample has gaussian distribution\n",
    "- H1: Sample does not have gaussian distribution\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18342710340675566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wehrm\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1394: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import normaltest\n",
    "\n",
    "## Normality test\n",
    "data = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = normaltest(data)\n",
    "\n",
    "print(p) # Fail to reject H0 --> Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anderson-Darling Test**\n",
    "\n",
    "Assumptions:<br>\n",
    "- Observations are i.i.d.\n",
    "\n",
    "Interpretation:<br>\n",
    "- H0: Sample has gaussian distribution\n",
    "- H1: Sample does not have gaussian distribution\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to reject H0 (gaussian) at 15.0% level\n",
      "Fail to reject H0 (gaussian) at 10.0% level\n",
      "Fail to reject H0 (gaussian) at 5.0% level\n",
      "Fail to reject H0 (gaussian) at 2.5% level\n",
      "Fail to reject H0 (gaussian) at 1.0% level\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import anderson\n",
    "\n",
    "## Normality test\n",
    "data = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "result = anderson(data)\n",
    "\n",
    "## get critical value and significance level\n",
    "for i in range(len(result.critical_values)):\n",
    "    sl,cv = result.significance_level[i], result.critical_values[i]\n",
    "    if result.statistic < cv:\n",
    "        print(\"Fail to reject H0 (gaussian) at %.1f%% level\" %(sl))\n",
    "    else:\n",
    "        print(\"Reject H0 (not gaussian) at %.1f%% level\" %(sl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Variable Relationship Tests (Correlation)\n",
    "\n",
    "This sub-chapter lists statistical tests to check if two samples are related and gives the Python code to do so.\n",
    "\n",
    "**Pearson's Correlation Coefficient**\n",
    "\n",
    "Tests whether two samples have a linear relationship.<br>\n",
    "\n",
    "Assumptions:<br>\n",
    "- Observations are i.i.d.\n",
    "- Both samples are normally distributed\n",
    "- Both samples have the same variance\n",
    "\n",
    "Interpretation:<br>\n",
    "- H0: Samples are independent\n",
    "- H1: Samples are dependent\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027872969514496193\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "## Test linear correlation\n",
    "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "data2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\n",
    "stat, p = pearsonr(data1, data2)\n",
    "\n",
    "print(p) # Reject H0 at 5% level --> samples are dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spearman's Rank Correlation**\n",
    "\n",
    "Tests whether two samples have a monotonic relationship (positive increase in X causes positive increase in Y and vice versa - positive correlation).<br>\n",
    "\n",
    "Assumptions:<br>\n",
    "- Observations are i.i.d.\n",
    "- Observations in each sample can be ranked\n",
    "\n",
    "Interpretation:<br>\n",
    "- H0: Samples are independent\n",
    "- H1: Samples are dependent\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016368033159867143\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "## Test monotonic correlation\n",
    "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "data2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\n",
    "stat, p = spearmanr(data1, data2)\n",
    "\n",
    "print(p) # Reject H0 --> Monotonic relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kendall's Rank Correlation**\n",
    "\n",
    "Tests whether two samples have a monotonic relationship (see above).<br>\n",
    "\n",
    "Assumptions:<br>\n",
    "- Observations are i.i.d.\n",
    "- Observations in each sample can be ranked\n",
    "\n",
    "Interpretation:<br>\n",
    "- H0: Samples are independent\n",
    "- H1: Samples are dependent\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0031612220209069567\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "## Test monotonic correlation\n",
    "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "data2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\n",
    "stat, p = kendalltau(data1, data2)\n",
    "\n",
    "print(p) # Reject H0 --> Monotonic relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chi-Squared Test**\n",
    "\n",
    "Tests whether two **categorical** variables are related or independent.<br>\n",
    "\n",
    "Assumptions:<br>\n",
    "- Obsrevations used in the calculation of the contingency table are independent\n",
    "- 25 or more examples in each cell of the contigency table\n",
    "\n",
    "Interpretation:<br>\n",
    "- H0: Samples are independent\n",
    "- H1: Samples are dependent\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873028283380073\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "## Test dependency\n",
    "table = [[10, 20, 30],[6,  9,  17]]\n",
    "stat, p, dof, expected = chi2_contingency(table)\n",
    "\n",
    "print(p) # Cannot reject H0 --> independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Compare Sample Means (Parametric)\n",
    "\n",
    "Parametric statistical tests can be used to compare data samples (e.g., to proof that they are from the same population).<br>\n",
    "\n",
    "**Student's t-test**\n",
    "\n",
    "Tests whether the means of two independent samples are significantly different.\n",
    "\n",
    "Assumptions:<br>\n",
    "- Observations are i.i.d.\n",
    "- Observations in each sample are normally distributed\n",
    "- Observations in each sample have the same variance\n",
    "\n",
    "Intepretation:<br>\n",
    "- H0: Means are equal\n",
    "- H1: Means are unequal\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7484698873615687\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "## Test for equal means\n",
    "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "stat, p = ttest_ind(data1, data2)\n",
    "\n",
    "print(p) # Cannot reject H0 --> means are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paired Student's t-test**\n",
    "\n",
    "Tests whether the means of two paired samples are significantly different. Paired samples are samples where each point in one sample matches another point in the second sample. For example, in a before-after drug test, the researcher looks at patients. So, each before datapoint is related to exactly one after datapoint.<br>\n",
    "\n",
    "Assumptions:<br>\n",
    "- Observations are i.i.d.\n",
    "- Observations are normally distributed\n",
    "- Observations in each sample have the same variance\n",
    "- Observations across each sample are paired\n",
    "\n",
    "Interpretation:<br>\n",
    "- H0: Means are equal\n",
    "- H1: Means are unequal\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7459078283577478\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "## Testing for equal means\n",
    "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "stat, p = ttest_rel(data1, data2)\n",
    "\n",
    "print(p) # Cannot reject H0 --> equal means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of Variance Test (ANOVA)**\n",
    "\n",
    "Tests whether the means of two or more independent samples are significantly different.<br>\n",
    "\n",
    "Assumptions:<br>\n",
    "- Observations are i.i.d.\n",
    "- Observations are normally distributed\n",
    "- Observations in each sample have the same variance\n",
    "\n",
    "Interpretation:<br>\n",
    "- H0: Means of the samples are equal\n",
    "- H1: One or more means of the samples are unequal\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9083957433926546\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "## Test for multiple equal means\n",
    "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "data3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\n",
    "stat, p = f_oneway(data1, data2, data3)\n",
    "\n",
    "print(p) # Cannot reject H0 --> sample means are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a **Repeated Measures ANOVA Test**, which tests whether the means of two or more paired samples are significantly different. However, unfortunately there exists no package for this test in Python as of now.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Compare Sample Means (Non-Parametric)\n",
    "\n",
    "XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Confidence Intervals\n",
    "\n",
    "Confidence interval is another way of valuing a distance between $\\bar{x}$ and $\\mu$. Under $H_{0}$ the random variable $(\\bar{X}-\\mu)\\sim Norm(0,\\frac{\\sigma}{\\sqrt{n}})$. Thus $\\displaystyle P\\{|\\bar{X}-\\mu|<=1.96\\frac{\\sigma}{\\sqrt{n}}\\}\\approx0.95$<br>\n",
    "\n",
    "**Definition**:<br>\n",
    "An interval $(\\bar{x}-a_{1},\\bar{x}+a_{1})$ such that $P\\{\\bar{X}-1.96\\frac{\\sigma}{\\sqrt{n}}<=\\mu<=\\bar{X}+1.96\\frac{\\sigma}{\\sqrt{n}}\\}=1-\\alpha$ is called an $(1-\\alpha)$-level confidence interval for the mean.<br>\n",
    "\n",
    "In applied machine learning, we may wish to use confidence intervals in the presentation of the skill of a predictive model. For example, a confidence interval could be used in presenting the skill of a classification model, which could be stated as:<br>\n",
    "Given the sample, there is a 95% likelihood that the range x to y covers the true model accuracy.<br>\n",
    "or<br>\n",
    "The accuracy of the model was x +/- y at the 95% confidence level.<br>\n",
    "\n",
    "Confidence intervals can also be used in the presentation of the error of a regression predictive model; for example:<br>\n",
    "There is a 95% likelihood that the range x to y covers the true error of the model.<br>\n",
    "or<br>\n",
    "The error of the model was x +/- y at the 95% confidence level.<br>\n",
    "\n",
    "These estimates of uncertainty help in two ways. First, the intervals give the consumers of the model an understanding about how good or bad the model may be. […] In this way, the confidence interval helps gauge the weight of evidence available when comparing models. The second benefit of the confidence intervals is to facilitate trade-offs between models. If the confidence intervals for two models significantly overlap, this is an indication of (statistical) equivalence between the two and might provide a reason to favor the less complex or more interpretable model.\n",
    "\n",
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.111\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# binomial confidence interval\n",
    "## Calcualting interval for classification error/accuracy: interval = z * sqrt( (error * (1 - error)) / n)\n",
    "interval = 1.96 * sqrt( (0.2 * (1 - 0.2)) / 50)\n",
    "print('%.3f' % interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the classification error of the model is 20% +/- 11% and the true classification error of the model is likely between 9% and 31%.<br>\n",
    "\n",
    "The proportion_confint() statsmodels function an implementation of the binomial proportion confidence interval.\n",
    "\n",
    "By default, it makes the Gaussian assumption for the Binomial distribution, although other more sophisticated variations on the calculation are supported. The function takes the count of successes (or failures), the total number of trials, and the significance level as arguments and returns the lower and upper bound of the confidence interval.\n",
    "\n",
    "The example below demonstrates this function in a hypothetical case where a model made 88 correct predictions out of a dataset with 100 instances and we are interested in the 95% confidence interval (provided to the function as a significance of 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower=0.816, upper=0.944\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "lower, upper = proportion_confint(88, 100, 0.05)\n",
    "print('lower=%.3f, upper=%.3f' % (lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower and upper bounds of the models accuracy.<br>\n",
    "\n",
    "**Nonparametric Confidence Interval**\n",
    "\n",
    "The predicted variable sometimes is not normally distributed, and even when it is, the variance of the normal distribution might not be equal at all levels of the predictor value. Alternatively, we may not know the analytical way to calculate a confidence interval for a skill score.<br>\n",
    "\n",
    "In these cases, the bootstrap resampling method can be used as a nonparametric method for calculating confidence intervals, nominally called bootstrap confidence intervals.\n",
    "\n",
    "The bootstrap is a simulated Monte Carlo method where samples are drawn from a fixed finite dataset with replacement and a parameter is estimated on each sample. This procedure leads to a robust estimate of the true population parameter via sampling.\n",
    "\n",
    "In this example, random values are drawn and the mean is calculated. Instead of the mean, this might just as well be a model's accuracy that is being fitted on the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50th percentile (median) = 0.750\n",
      "2.5th percentile = 0.741\n",
      "97.5th percentile = 0.757\n"
     ]
    }
   ],
   "source": [
    "# bootstrap confidence intervals\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from numpy import mean\n",
    "from numpy import median\n",
    "from numpy import percentile\n",
    "\n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "# generate dataset\n",
    "dataset = 0.5 + rand(1000) * 0.5\n",
    "# bootstrap\n",
    "scores = list()\n",
    "for _ in range(100):\n",
    "\t# bootstrap sample\n",
    "\tindices = randint(0, 1000, 1000)\n",
    "\tsample = dataset[indices]\n",
    "\t# calculate and store statistic\n",
    "\tstatistic = mean(sample)\n",
    "\tscores.append(statistic)\n",
    "print('50th percentile (median) = %.3f' % median(scores))\n",
    "# calculate 95% confidence intervals (100 - alpha)\n",
    "alpha = 5.0\n",
    "# calculate lower percentile (e.g. 2.5)\n",
    "lower_p = alpha / 2.0\n",
    "# retrieve observation at lower percentile\n",
    "lower = max(0.0, percentile(scores, lower_p))\n",
    "print('%.1fth percentile = %.3f' % (lower_p, lower))\n",
    "# calculate upper percentile (e.g. 97.5)\n",
    "upper_p = (100 - alpha) + (alpha / 2.0)\n",
    "# retrieve observation at upper percentile\n",
    "upper = min(1.0, percentile(scores, upper_p))\n",
    "print('%.1fth percentile = %.3f' % (upper_p, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the mean is likely to be between 0.741 and 0.757 (95% confidence interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:<br>\n",
    "[1] https://machinelearningmastery.com/confidence-intervals-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
